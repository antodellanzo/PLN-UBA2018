Ejercicio 1

Para este ejercicio completé los métodos del script stats.py viendo qué información debería guardar en el constructor de las 'tagged_sents' recibidas y en qué estructuras de datos para facilitar la devolución de las estadísticas correspondientes. Luego de eso, lo ejecuté, obteniendo los siguientes resutlados sobre las etiquetas más frecuentes:

Most Frequent POS Tags
======================
tag		freq	%	top
sp000	79884	15.45	(de, en, a, del, con)
nc0s000	63452	12.27	(presidente, equipo, partido, país, año)
da0000	54549	10.55	(la, el, los, las, El)
aq0000	33906	6.56	(pasado, gran, mayor, nuevo, próximo)
fc		30147	5.83	(,)
np00000	29111	5.63	(Gobierno, España, PP, Barcelona, Madrid)
nc0p000	27736	5.36	(años, millones, personas, países, días)
fp		17512	3.39	(.)
rg		15336	2.97	(más, hoy, también, ayer, ya)
cc		15023	2.90	(y, pero, o, Pero, e)

en donde cada etiqueta significa lo siguiente:
tag		significado
sp000	preposición
nc0s000	sustantivo singular
da0000	artículo
aq0000	adjetivo descriptivo
fc		coma
np00000	sustantivo propio
nc0p000	sutantivo plural
fp		punto
rg		adverbio
cc		conjunción

Para los niveles de ambigüedad se obtuvo lo siguiente:

Word Ambiguity Levels
=====================
n	words	%		top 5
1	43972	94.56	(,, con, por, su, El)
2	2318	4.98	(el, en, y, ", los)
3	180		0.39	(de, la, ., un, no)
4	23		0.05	(que, a, dos, este, fue)
5	5		0.01	(mismo, cinco, medio, ocho, vista)
6	3		0.01	(una, como, uno)
7	0		0.00	()
8	0		0.00	()
9	0		0.00	()

Ejercicio 2

Para este ejercicio, completé el constructor de la clase BaselineTagger teniendo en cuenta qué datos y en cuáles estructuras de datos me convenía guardar la información recibida como parámetros de entrada para poder implementar los métodos de 'tag_word' y 'unknown'. Para esto, me guardé un diccionario que, para cada palabra, guarda un diccionario que contiene cada uno de los tags asociados a esa palabra y la cuenta de los mismos. También almacené en una variable el default_tag para utilizarlo al momento de devolver el tag de una palabra y que ésta sea desconocida. Luego de esto, procedí a entrenar y evaluar éste baseline, obteniendo los siguientes resultados:

					|  accuracy	| score over known words | score over unknown words	|  tiempo
baseline tagger		| 	 87.60% | 				  95.29% | 					 18.01%	|	0.4''

Matriz de confusion
g \ m	sp000	nc0s000	da0000	aq0000	fc	nc0p000	rg		np00000	fp	cc
sp000	14.28	0.05	-		-	-	-	0.01	-		-		-	
nc0s000	0.00	12.24	-		0.23	-	0.00	0.03	0.00	-	0.00	
da0000	-		0.15	9.54	-	-	-	-		-		-		-	
aq0000	0.01	2.06	-		4.81	-	0.14	0.00	-		-	-	
fc		-		-	-	-		5.85	-	-		-		-		-	
nc0p000	-		1.24	-		0.18	-	4.10	-		-		-	-	
rg		0.02	0.31	-		0.03	-	-		3.28	-		-	0.02	
np00000	0.00	2.05	-		0.00	-	0.00	-		1.52	-	0.00	
fp		-		-	-	-		-		-	-		-		3.55	-	
cc		0.00	0.01	-		-		-	-		0.05	0.00	-	3.34

Ejercicio 3

Para este ejercicio simplemente completé cada unos de los métodos del script features retornando la información requerida sobre información de las historias.

Ejercicio 4

Para este ejercicio, en el constructor del script memm, creé el pipeline con un vector con todos los features generados en el ejercicio anterior y el clasificador recibido como parámetro, y luego almacené en una variable de instancia un set con todas las palabras conocidas para poder completar el método 'unknown' que devuelve un booleano indicando si la palabra es conocida en el modelo. Luego, completé el método 'tag_history', que dada una historia predice según lo entrenado en el pipeline los tags correspondientes a dicha historia y lo retorna. Finalmente, completé el método 'tag' que retorna todos los tags asociados a una oración. Para esto, se va iterando sobre la oración de entrada, generando historias y luego prediciendo el tag de la misma. 
Modifiqué el script train para poder entrenar con este nuevo modelo y entrené modelos para cada valor de n \in {1, 2, 3, 4} y para cada uno de los clasificadores (LogisticRegression, MultinomialNB y LinearSVC). Luego, modifiqué el script eval para poder calcular el tiempo que tarda en evaluarse el modelo y lo imprimí. Obtuve los siguientes resultados para cada clasificador:

Logistic regression	
(adjunto archivo 'logisticRegressionMemmEvalReadme' con los resultados de la matriz de confusión para cada valor de n para este modelo)
n	|  accuracy	| score over known words | score over unknown words	|	tiempo 
1	|	 91.69% | 				  95.00% | 					 61.69% |	27.5''
2	| 	 90.69% | 				  93.99% | 					 60.76% |	29.0''
3	|	 91.42% | 				  94.67% | 					 61.99% |	30.4''	
4	|	 90.60% | 				  93.93% | 					 60.44% |	32.0''	

MultinomialNB
n	|  accuracy	| score over known words | score over unknown words	|	  tiempo 
1	|	 74.85% | 				  77.99% | 					 46.45% |	3225.7''

LinearSVC
n	|  accuracy	| score over known words | score over unknown words	|	tiempo 
1	|	 94.11% | 				  97.57% | 					 62.77% |	26.7''
2	|	 93.71% | 				  97.24% | 					 61.68% |	32.5''
3	|	 94.21% | 				  97.55% | 					 63.89% |	32.2''
4	|	 93.70% | 				  97.26% | 					 61.49% |	33.4''



